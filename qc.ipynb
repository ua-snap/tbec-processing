{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94be9f4-b5f4-462f-b411-3ef711ee82e1",
   "metadata": {},
   "source": [
    "# Quality control on indices\n",
    "\n",
    "This notebook performs some quality control on the data products produced in the CORDEX climate indices data pipeline.\n",
    "\n",
    "Run the cell below to set up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b9ef74-b021-4bec-9088-0b4ab770c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# project\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ddbf9-b29c-4c76-a4e6-e1569508bfdf",
   "metadata": {},
   "source": [
    "## Annual indices\n",
    "\n",
    "This section will spot check the dataset of indices derived at an annual time scale, which are saved to the `annual_indices.nc` file.\n",
    "\n",
    "We will look at some test locations and sport check calcualtions for all indices that are currently computed.\n",
    "\n",
    "Define some locations and years to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0956f5-7307-4e70-89b1-18366ac9384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_coords = [(-146.3499, 61.1309), (-156.7886, 71.2906)] # (lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c3218-826c-468b-85ce-b11d6f58309f",
   "metadata": {},
   "source": [
    "Open connection to annual indices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7abc7ec2-f413-4022-9666-4d02f8f29931",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(indices_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208a959-75a9-4422-a771-dfe725411608",
   "metadata": {},
   "source": [
    "Set a couple year-model-scenario combinations to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d721a8-5a17-47b0-bee7-ecedb32c44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary\n",
    "check_args = [\n",
    "    # (model, scenario, year)\n",
    "    {\"model\": \"NCC-NorESM1-M_SMHI-RCA4\", \"scenario\": \"hist\", \"year\": 1995},\n",
    "    {\"model\": \"MPI-M-MPI-ESM-LR_SMHI-RCA4-SN\", \"scenario\": \"rcp85\", \"year\": 2095},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb0c21-47db-4b59-84a5-34984f7e68d3",
   "metadata": {},
   "source": [
    "And define a function perform the task of extracting the cordex data from `cordex_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fbec12-1dbc-4030-a265-f8400d09e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cordex(args, varname, coords):\n",
    "    model, scenario, year = list(args.values())\n",
    "    base_fp = cordex_dir.joinpath(scenario, varname, temp_fn.format(scenario, varname, model))\n",
    "    with xr.open_dataset(base_fp) as cdx_ds:\n",
    "        da = (\n",
    "            cdx_ds[varname]\n",
    "            .sel(time=slice(f\"{year}-01-01\", f\"{year}-12-31\"))\n",
    "            .sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        )\n",
    "    \n",
    "    return da.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee8d2a-c1a1-4c44-8071-c6cfb2e9f401",
   "metadata": {},
   "source": [
    "We are now ready to begin checking the values.\n",
    "\n",
    "Each of the below code blocks include assertions for each of the coordinate / `args` combinations. If they run without exception, then we can consider that a passing test.\n",
    "\n",
    "#### `rx1day` - max 1-day precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d54a88-e11b-4516-8a0c-fd48e165a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"pr\"\n",
    "index = \"rx1day\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        arr = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        calc = round(np.max(arr) * 86400, 1)\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eea041-28c0-45ce-9658-69c7a2004c10",
   "metadata": {},
   "source": [
    "#### `hsd` - heavy snow days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92abdcd-8c1c-4101-b2df-ca4dae9c2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"prsn\"\n",
    "index = \"hsd\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        arr = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        calc = ((arr * 8640) > 10).sum()\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99356b9f-fac9-46f0-849e-afb5107e369b",
   "metadata": {},
   "source": [
    "#### `hd` - hot day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125a19f9-4978-4c5c-a156-3dd583523d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"tasmax\"\n",
    "index = \"hd\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        arr = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        calc = np.round(np.sort(arr)[-6] - 273.15, 1)\n",
    "        # dang floats being weird..\n",
    "        assert np.isclose(calc, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c19217-1d8e-481c-808c-8e2aa60f4890",
   "metadata": {},
   "source": [
    "#### `cd` - cold day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f541cf1-af2a-460a-8fd6-fda6c0299e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"tasmin\"\n",
    "index = \"cd\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        arr = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        calc = np.round(np.sort(arr)[5] - 273.15, 1)\n",
    "        # dang floats being weird..\n",
    "        assert np.isclose(calc, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1dfdc-436e-428c-b704-2fab92811d38",
   "metadata": {},
   "source": [
    "## Location extrations\n",
    "\n",
    "This section will validate the decadal and era-based extractions from the annual indices dataset made at select locations and saved to excel and CSV format.\n",
    "\n",
    "We will just rely on the same model-scenario combinations as above for simplicity, but we will then check the aggregated values - min, mean and max - for all available summary periods, for both types of temporal summaries. Run this monster loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "324b4204-a076-4f37-9d6b-7594f5e849db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the index variable names available from config\n",
    "index_list = [name for index_list in idx_varname_lu.values() for name in index_list]\n",
    "for loc in [\"Kaktovik\", \"Ketchikan\"]:\n",
    "    lat, lon = locations[loc]\n",
    "    for args in check_args:\n",
    "        model, scenario, _ = args\n",
    "        era_df = pd.read_excel(idx_era_summary_fp, loc)\n",
    "        decade_df = pd.read_excel(idx_decade_summary_fp, loc) \n",
    "        for index in index_list:\n",
    "            query_str = f\"model == '{model}' & scenario == '{scenario}' & idx_var == '{index}'\"\n",
    "            # check era summaries\n",
    "            temp_df = era_df.query(query_str)\n",
    "            eras = np.unique(temp_df.era)\n",
    "            for era in eras:\n",
    "                start_year, end_year = era.split(\"-\")\n",
    "                arr = ds[index].sel(lat=lat, lon=lon, method=\"nearest\").sel(\n",
    "                    model=model, scenario=scenario, year=slice(start_year, end_year)\n",
    "                )\n",
    "                assert arr.mean() == temp_df.query(f\"era == {era}\")[\"mean\"]\n",
    "                assert arr.min() == temp_df.query(f\"era == {era}\")[\"min\"]\n",
    "                assert arr.max() == temp_df.query(f\"era == {era}\")[\"max\"]\n",
    "                \n",
    "            temp_df = decade_df.query(query_str)\n",
    "            decades = np.unique(temp_df.decade)\n",
    "            for decade in decades:\n",
    "                start_year, end_year = decade.split(\"-\")\n",
    "                arr = ds[index].sel(lat=lat, lon=lon, method=\"nearest\").sel(\n",
    "                    model=model, scenario=scenario, year=slice(start_year, end_year)\n",
    "                )\n",
    "                assert arr.mean() == temp_df.query(f\"decade == {era}\")[\"mean\"]\n",
    "                assert arr.min() == temp_df.query(f\"decade == {era}\")[\"min\"]\n",
    "                assert arr.max() == temp_df.query(f\"decade == {era}\")[\"max\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5fc68-3215-4ee5-8e69-c7ea6d71f007",
   "metadata": {},
   "source": [
    "And if there are no errors, we've verified all summaries for two model-scenario combinations, for two locations.\n",
    "\n",
    "That concludes the validation of data products. Close the connection to the annual indices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f61310-6647-4af8-8fba-88a0e309eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
