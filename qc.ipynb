{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94be9f4-b5f4-462f-b411-3ef711ee82e1",
   "metadata": {},
   "source": [
    "# Quality control on indices\n",
    "\n",
    "This notebook performs some quality control on the data products produced in the CORDEX climate indices data pipeline.\n",
    "\n",
    "Run the cell below to set up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b9ef74-b021-4bec-9088-0b4ab770c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xclim.indices as xci\n",
    "from xclim.core.calendar import percentile_doy\n",
    "from xclim.core.units import convert_units_to, to_agg_units\n",
    "from xclim.indices.generic import threshold_count\n",
    "# project\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ddbf9-b29c-4c76-a4e6-e1569508bfdf",
   "metadata": {},
   "source": [
    "## Annual indices\n",
    "\n",
    "This section will spot check the dataset of indices derived at an annual time scale, which are saved to the `annual_indices.nc` file.\n",
    "\n",
    "We will look at some test locations and spot check calculations for all indices that are currently computed.\n",
    "\n",
    "Define some locations to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0956f5-7307-4e70-89b1-18366ac9384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_coords = [(-146.3499, 61.1309), (-156.7886, 71.2906)] # (lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208a959-75a9-4422-a771-dfe725411608",
   "metadata": {},
   "source": [
    "Set a couple year-model-scenario combinations to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d721a8-5a17-47b0-bee7-ecedb32c44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary\n",
    "check_args = [\n",
    "    # (model, scenario, year)\n",
    "    {\"model\": \"NCC-NorESM1-M_SMHI-RCA4\", \"scenario\": \"hist\", \"year\": 1995},\n",
    "    {\"model\": \"MPI-M-MPI-ESM-LR_SMHI-RCA4-SN\", \"scenario\": \"rcp85\", \"year\": 2095},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c3218-826c-468b-85ce-b11d6f58309f",
   "metadata": {},
   "source": [
    "Open connection to annual indices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abc7ec2-f413-4022-9666-4d02f8f29931",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(indices_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb0c21-47db-4b59-84a5-34984f7e68d3",
   "metadata": {},
   "source": [
    "And define a function to perform the task of extracting the cordex data from `cordex_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fbec12-1dbc-4030-a265-f8400d09e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cordex(args, varname, coords):\n",
    "    model, scenario, time = list(args.values())\n",
    "    if type(time) == int:\n",
    "        time_sl = slice(f\"{time}-01-01\", f\"{time}-12-31\")\n",
    "    elif type(time) == slice:\n",
    "        time_sl = time\n",
    "    base_fp = cordex_dir.joinpath(scenario, varname, temp_fn.format(scenario, varname, model))\n",
    "    with xr.open_dataset(base_fp) as cdx_ds:\n",
    "        da = (\n",
    "            cdx_ds[varname]\n",
    "            .sel(time=time_sl)\n",
    "            .sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        )\n",
    "    \n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee8d2a-c1a1-4c44-8071-c6cfb2e9f401",
   "metadata": {},
   "source": [
    "We are now ready to begin checking the values.\n",
    "\n",
    "Each of the below code blocks include assertions for each of the coordinate / `args` combinations. If they run without exception, then we can consider that a passing test.\n",
    "\n",
    "#### `rx1day` - max 1-day precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d54a88-e11b-4516-8a0c-fd48e165a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"pr\"\n",
    "index = \"rx1day\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        da = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        # convert to mm from flux using conversion factor of 8640 provided by John W (for kg m-2 s-1 to cm, * 10 for mm)\n",
    "        calc = round(np.max(da.values) * 86400, 1)\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b58238-48e3-42a8-9b45-2cd7c02cdafe",
   "metadata": {},
   "source": [
    "#### `rx5day` - max 5-day precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef04a70-89e4-4166-9209-a6150da13af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"pr\"\n",
    "index = \"rx5day\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        da = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        # convert to mm from flux using conversion factor of 8640 provided by John W (for kg m-2 s-1 to cm, * 10 for mm)\n",
    "        calc = round(np.max(da.rolling(time=5).sum()).values * 86400, 1)\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def55bf6-f3ab-4860-a978-82252227f1e0",
   "metadata": {},
   "source": [
    "#### `r10mm` - Heavy precip days\n",
    "\n",
    "Number of days in a year with over 10mm of precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c106e9-9a7b-4f59-a9c5-1386c351402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"pr\"\n",
    "index = \"r10mm\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        da = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        # convert to mm from flux using conversion factor of 8640 provided by John W (for kg m-2 s-1 to cm, * 10 for mm)\n",
    "        calc = ((da.values * 86400) > 10).sum()\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac1ebd-bc64-4b89-a6e6-18bccc6242a8",
   "metadata": {},
   "source": [
    "#### `cwd` - Maximum consecutive wet days\n",
    "\n",
    "Threshold of 1mm / day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81948fa-21c4-4a37-aac0-2b55465eba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"pr\"\n",
    "index = \"cwd\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        da = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        # wet days\n",
    "        wd_arr = (da * 86400) > 1\n",
    "        # ugly but this is finding the max sequence of wet days\n",
    "        acc = []\n",
    "        seq = []\n",
    "        for x in wd_arr.values:\n",
    "            if x:\n",
    "                seq.append(x)\n",
    "            else:\n",
    "                acc.append(seq)\n",
    "                seq=[]\n",
    "        calc = np.max([len(seq) for seq in acc])\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb6928-190d-4bdc-a470-215b4e71d267",
   "metadata": {},
   "source": [
    "#### `cdd` - Maximum consecutive dry days\n",
    "\n",
    "Threshold of 1mm / day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21fe1875-6a74-47cb-97f0-de0a2fbc66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"pr\"\n",
    "index = \"cdd\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        da = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        # dry days\n",
    "        dd_arr = (da * 86400) < 1\n",
    "        # ugly but this is finding the max sequence of dry days\n",
    "        acc = []\n",
    "        seq = []\n",
    "        for x in dd_arr.values:\n",
    "            if x:\n",
    "                seq.append(x)\n",
    "            else:\n",
    "                acc.append(seq)\n",
    "                seq=[]\n",
    "        calc = np.max([len(seq) for seq in acc])\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6fa7d-cb50-4e7e-8adb-1d63c39dde09",
   "metadata": {},
   "source": [
    "#### `wndd` - Maximum consecutive windy days\n",
    "\n",
    "Threshold of 10 m/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "412f566d-3bc3-41af-b42b-f965e9dc2c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"sfcWind\"\n",
    "index = \"wndd\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        da = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        # windy days\n",
    "        wnd_arr = da > 10\n",
    "        # ugly but this is finding the max sequence of windy days\n",
    "        acc = []\n",
    "        seq = []\n",
    "        for x in wnd_arr.values:\n",
    "            if x:\n",
    "                seq.append(x)\n",
    "            else:\n",
    "                acc.append(seq)\n",
    "                seq=[]\n",
    "        calc = np.max([len(seq) for seq in acc])\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eea041-28c0-45ce-9658-69c7a2004c10",
   "metadata": {},
   "source": [
    "#### `hsd` - heavy snow days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a92abdcd-8c1c-4101-b2df-ca4dae9c2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"prsn\"\n",
    "index = \"hsd\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        da = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        # convert to cm from flux using conversion factor of 8640 provided by John W (for kg m-2 s-1 to cm)\n",
    "        calc = np.round(np.sort(da.values)[-5:].mean() * 8640, 1).astype(np.float32)\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99356b9f-fac9-46f0-849e-afb5107e369b",
   "metadata": {},
   "source": [
    "#### `hd` - hot day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125a19f9-4978-4c5c-a156-3dd583523d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"tasmax\"\n",
    "index = \"hd\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        arr = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        calc = np.round(np.sort(arr)[-6] - 273.15, 1).astype(np.float32)\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c19217-1d8e-481c-808c-8e2aa60f4890",
   "metadata": {},
   "source": [
    "#### `cd` - cold day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f541cf1-af2a-460a-8fd6-fda6c0299e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"tasmin\"\n",
    "index = \"cd\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        arr = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        calc = np.round(np.sort(arr)[5] - 273.15, 1).astype(np.float32)\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525f9c4-1331-45a3-b6ff-112aef3a5afd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `wsdi` - Warm spell duration index\n",
    "\n",
    "Number of days inside spells of a minimum number of consecutive days where the daily maximum temperature is above the 90th percentile. The 90th percentile should be computed for a 5-day moving window, centered on each calendar day in the 1976-2005 period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a80af1f-6543-4828-b83e-2996e7480b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"tasmax\"\n",
    "index = \"wsdi\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        tasmax = extract_cordex(args, varname, coords)\n",
    "        if args[\"scenario\"] != \"hist\":\n",
    "            hist_args = args.copy()\n",
    "            hist_args.update({\"scenario\": \"hist\", \"year\": slice(\"1976-01-01\", \"2005-12-31\")})\n",
    "            tasmax_hist = extract_cordex(hist_args, varname, coords)\n",
    "        else:\n",
    "            tasmax_hist = tasmax\n",
    "        tasmax_per = percentile_doy(tasmax_hist, per=90).sel(percentiles=90)\n",
    "        calc = xci.warm_spell_duration_index(tasmax, tasmax_per, window=6, freq=\"YS\").drop(\"percentiles\")\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2aed3e-98f0-4c5b-9fc6-9892ce519003",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `csdi` - Cold spell duration index\n",
    "\n",
    "Number of days inside spells of a minimum number of consecutive days where the daily minimum temperature is below the 10th percentile. The 10th percentile should be computed for a 5-day moving window, centered on each calendar day in the 1976-2005 period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bdfa7ab-59c0-4a2b-9f99-09afc956e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"tasmin\"\n",
    "index = \"csdi\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        tasmin = extract_cordex(args, varname, coords)\n",
    "        if args[\"scenario\"] != \"hist\":\n",
    "            hist_args = args.copy()\n",
    "            hist_args.update({\"scenario\": \"hist\", \"year\": slice(\"1976-01-01\", \"2005-12-31\")})\n",
    "            tasmin_hist = extract_cordex(hist_args, varname, coords)\n",
    "        else:\n",
    "            tasmin_hist = tasmin\n",
    "        tasmin_per = percentile_doy(tasmin_hist, per=10).sel(percentiles=10)\n",
    "        calc = xci.cold_spell_duration_index(tasmin, tasmin_per, window=6, freq=\"YS\").drop(\"percentiles\")\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c001c-33b8-4d8f-8d63-ff4c5bffe45c",
   "metadata": {},
   "source": [
    "#### `su` - Summer days\n",
    "\n",
    "Number of days above 25C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8253d77-fc26-4a25-89c4-0396d753c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"tasmax\"\n",
    "index = \"su\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        arr = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        calc = ((arr - 273.15) > 25).sum()\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6690e-2815-4398-b413-b9297555ad68",
   "metadata": {},
   "source": [
    "#### `cd` - Deep winter days\n",
    "\n",
    "Number of days below -30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27c5aa13-9294-430d-9623-fd9542a47d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"tasmin\"\n",
    "index = \"dw\"\n",
    "for args in check_args:\n",
    "    for coords in check_coords:\n",
    "        arr = extract_cordex(args, varname, coords)\n",
    "        test = ds[index].sel(args).sel(lat=coords[0], lon=coords[1], method=\"nearest\")\n",
    "        calc = ((arr - 273.15) < -30).sum()\n",
    "        assert calc == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1dfdc-436e-428c-b704-2fab92811d38",
   "metadata": {},
   "source": [
    "## Location extractions\n",
    "\n",
    "This section will validate the decadal and era-based extractions from the annual indices dataset made at select locations and saved to excel and CSV format.\n",
    "\n",
    "We will just rely on the same model-scenario combinations as above for simplicity, but we will then check the aggregated values - min, mean and max - for all available summary periods, for both types of temporal summaries. Run this monster loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "324b4204-a076-4f37-9d6b-7594f5e849db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the index variable names available from config\n",
    "index_list = [name for index_list in idx_varname_lu.values() for name in index_list]\n",
    "for loc in [\"Kaktovik\", \"Ketchikan\"]:\n",
    "    lat, lon = locations[loc]\n",
    "    for args in check_args:\n",
    "        model, scenario, _ = args\n",
    "        era_df = pd.read_excel(idx_era_summary_fp, loc)\n",
    "        decade_df = pd.read_excel(idx_decade_summary_fp, loc) \n",
    "        for index in index_list:\n",
    "            query_str = f\"model == '{model}' & scenario == '{scenario}' & idx_var == '{index}'\"\n",
    "            # check era summaries\n",
    "            temp_df = era_df.query(query_str)\n",
    "            eras = np.unique(temp_df.era)\n",
    "            for era in eras:\n",
    "                start_year, end_year = era.split(\"-\")\n",
    "                arr = ds[index].sel(lat=lat, lon=lon, method=\"nearest\").sel(\n",
    "                    model=model, scenario=scenario, year=slice(start_year, end_year)\n",
    "                )\n",
    "                assert arr.mean() == temp_df.query(f\"era == {era}\")[\"mean\"]\n",
    "                assert arr.min() == temp_df.query(f\"era == {era}\")[\"min\"]\n",
    "                assert arr.max() == temp_df.query(f\"era == {era}\")[\"max\"]\n",
    "                \n",
    "            temp_df = decade_df.query(query_str)\n",
    "            decades = np.unique(temp_df.decade)\n",
    "            for decade in decades:\n",
    "                start_year, end_year = decade.split(\"-\")\n",
    "                arr = ds[index].sel(lat=lat, lon=lon, method=\"nearest\").sel(\n",
    "                    model=model, scenario=scenario, year=slice(start_year, end_year)\n",
    "                )\n",
    "                assert arr.mean() == temp_df.query(f\"decade == {era}\")[\"mean\"]\n",
    "                assert arr.min() == temp_df.query(f\"decade == {era}\")[\"min\"]\n",
    "                assert arr.max() == temp_df.query(f\"decade == {era}\")[\"max\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5fc68-3215-4ee5-8e69-c7ea6d71f007",
   "metadata": {},
   "source": [
    "And if there are no errors, we've verified all summaries for two model-scenario combinations, for two locations.\n",
    "\n",
    "That concludes the validation of data products. Close the connection to the annual indices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8f61310-6647-4af8-8fba-88a0e309eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
