{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988f67bc-1803-46f1-8c97-eb51c850a0c7",
   "metadata": {},
   "source": [
    "# Process extreme weather dataset\n",
    "\n",
    "This notebook is used to derive a dataset of extreme events using the bias-corrected CORDEX data provided to Two Bears Environmental Consulting (via KR at SNAP) by Stantec, Inc.\n",
    "\n",
    "This dataset will consist of four \"extreme\" variables (to start), derived at the annual scale for all available models and scenarios on the same grid as the original bias-corrected data.\n",
    "\n",
    "The dataset will be a datacube with the following dimensions:\n",
    "\n",
    "* variable\n",
    "* model\n",
    "* scenario\n",
    "* year\n",
    "* Y\n",
    "* X\n",
    "\n",
    "## Variables\n",
    "\n",
    "The following variables are to be derived, at the annual scale:\n",
    "\n",
    "* `hd`:  “Hot day” threshold -- the highest observed daily $T_{max}$ such that there are 5 other observations equal to or greater than this value.\n",
    "* `cd`: “Cold day” threshold -- the lowest observed daily $T_{max}$ such that there are 5 other observations equal to or less than this value.\n",
    "* `rx1say`: Max 1-day precipitation –- maximum 1-day precipitation\n",
    "* `hsd`: Heavy Snow Days –- number of days with snowfall > 10 cm\n",
    "\n",
    "## Models\n",
    "\n",
    "The CORDEX data are created by combining a regional climate model with a global circulation model, and there are a couple different types of each represented in this dataset. The combinations are nowhere near exhaustive though, so for our purposes, it should be sufficient to just treat each unique combination as its own \"model\", of which there are 11:\n",
    "\n",
    "* CCCma-CanESM2 x CCCma-CanRCM4\n",
    "* CCCma-CanESM2 x SMHI-RCA4\n",
    "* CCCma-CanESM2 x UQAM-CRCM5\n",
    "* ICHEC-EC-EARTH x DMI-HIRHAM5\n",
    "* ICHEC-EC-EARTH x SMHI-RCA4\n",
    "* ICHEC-EC-EARTH x SMHI-RCA4-SN\n",
    "* MPI-M-MPI-ESM-LR x MGO-RRCM\n",
    "* MPI-M-MPI-ESM-LR x SMHI-RCA4\n",
    "* MPI-M-MPI-ESM-LR x SMHI-RCA4-SN\n",
    "* MPI-M-MPI-ESM-MR x UQAM-CRCM5\n",
    "* NCC-NorESM1-M x SMHI-RCA4\n",
    "\n",
    "## Processing\n",
    "\n",
    "Here we now derive this dataset. The strategy will be to iterate over the datasets and read / summarize into summary `xarray.DataArray` objects with matching dimensions, and then combined into one `xarray.DataSet` to then be saved as a netCDF. \n",
    "\n",
    "Note - optimizations were attempted with `multiprocessing.Pool` but things seemed to be locking up.\n",
    "\n",
    "Start with defining directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ade981-a210-4e66-88d9-452f5592de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# base directory for the project\n",
    "base_dir = Path(os.getenv(\"BASE_DIR\") or \"/workspace/Shared/Tech_Projects/TBEC_CMIP5_Processing/project_data/\")\n",
    "# actual subdirectory of base_dir that contains CORDEX data\n",
    "src_dir = base_dir.joinpath(\"arc_cordex/bias_corrected\")\n",
    "# write to the final_products/auxiliary_content directory, as the outputs from here will be used for reporting\n",
    "out_dir = Path(os.getenv(\"OUTPUT_DIR\") or \"/workspace/Shared/Tech_Projects/TBEC_CMIP5_Processing/final_products/\")\n",
    "out_ds_fp = out_dir.joinpath(\"annual_extremes.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2fa15-bfd3-45b7-a23a-5b004c0110c3",
   "metadata": {},
   "source": [
    "Models, scenarios, and variable names for iterating over file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99295aba-1f23-4eae-b096-50f6d0fc0954",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"CCCma-CanESM2_CCCma-CanRCM4\",\n",
    "    \"CCCma-CanESM2_SMHI-RCA4\",\n",
    "    \"CCCma-CanESM2_UQAM-CRCM5\",\n",
    "    \"ICHEC-EC-EARTH_DMI-HIRHAM5\",\n",
    "    \"ICHEC-EC-EARTH_SMHI-RCA4\",\n",
    "    \"ICHEC-EC-EARTH_SMHI-RCA4-SN\",\n",
    "    \"MPI-M-MPI-ESM-LR_MGO-RRCM\",\n",
    "    \"MPI-M-MPI-ESM-LR_SMHI-RCA4\",\n",
    "    \"MPI-M-MPI-ESM-LR_SMHI-RCA4-SN\",\n",
    "    \"MPI-M-MPI-ESM-MR_UQAM-CRCM5\",\n",
    "    \"NCC-NorESM1-M_SMHI-RCA4\",\n",
    "]\n",
    "\n",
    "scenarios = [\"hist\", \"rcp45\", \"rcp85\"]\n",
    "\n",
    "# not using all available model variables yet\n",
    "# varnames = [\"pr\", \"prsn\", \"sfcWind\", \"tas\", \"tasmax\", \"tasmin\"]\n",
    "varnames = [\"pr\", \"prsn\", \"tasmax\", \"tasmin\"]\n",
    "\n",
    "\n",
    "# map from varnames to desired summary variables\n",
    "aggr_varname_lu = {\n",
    "    \"pr\": [\"rx1say\"],\n",
    "    \"prsn\": [\"hsd\"],\n",
    "    \"tasmax\": [\"hd\"],\n",
    "    \"tasmin\": [\"cd\"],\n",
    "}\n",
    "\n",
    "# template filename\n",
    "temp_fn = \"ARC44_{}_{}_{}_ERA5bc.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e0457-0db7-44cd-90b3-9af1d905ec46",
   "metadata": {},
   "source": [
    "Create a list of arguments for filenames and requested summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7f792c-c633-400f-86c2-1eb99a25af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chunk is an artifact of multiprocessing-based optimization attempts\n",
    "#  but it still serves nicely for utilizing a tqdm progress bar in serial processing\n",
    "args = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for varname in varnames:\n",
    "        for model in models:\n",
    "            fp = src_dir.joinpath(scenario, varname, temp_fn.format(scenario, varname, model))\n",
    "            \n",
    "            # aggregate variable names for this particular file\n",
    "            aggr_varnames = aggr_varname_lu[varname]\n",
    "            \n",
    "            if fp.exists():\n",
    "                args.append((fp, aggr_varnames, varname, scenario, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5702d3-7c03-44be-95a5-054296645268",
   "metadata": {},
   "source": [
    "Define functions for each of the new extreme aggregate variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba3faa2-4a2c-4623-aaf6-42b148949dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def hd(arr, axis):\n",
    "    # np.sort defaults to ascending.. \n",
    "    #   hd is simply \"6th hottest\" day\n",
    "    return np.sort(arr, axis)[-6,:,:]\n",
    "    \n",
    "\n",
    "def cd(arr, axis):\n",
    "    # np.sort defaults to ascending.. \n",
    "    #   cd is simply \"6th coldest\" day\n",
    "    return np.sort(arr, axis)[5,:,:]\n",
    "\n",
    "\n",
    "def rx1say(arr, axis):\n",
    "    # bit redundant but here with others for completion\n",
    "    return np.max(arr, axis)\n",
    "\n",
    "\n",
    "def hsd(arr, axis):\n",
    "    # number of days with snowfall greater than 10cm\n",
    "    # assumes input is in cm\n",
    "    return (arr > 10).sum(axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fb579-840b-4aad-860b-d79c6af41020",
   "metadata": {},
   "source": [
    "a function for summarizing a `DataArray` of model output for a particular variable to create a new data array of an aggregate variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bea5adf-5001-43e3-a69e-fdaae9dabf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(da, aggr, model, scenario):\n",
    "    \"\"\"Summarize a DataArray according to a specified\n",
    "    aggregation function\n",
    "    \n",
    "    Args:\n",
    "        da (xarray.DataArray): the DataArray object containing the base variable\n",
    "            data to b summarized according to aggr\n",
    "        aggr (str): String corresponding to the type of aggregation to do, \n",
    "            based on the new aggregation functions\n",
    "        scenario (str): scenario being run (for new coordinate dimension)\n",
    "        model (str): model being run (for new coordinate dimension)\n",
    "            \n",
    "    Returns:\n",
    "        A new data array with dimensions year, latitude, longitude, in that order\n",
    "            containing the summarized information\n",
    "    \"\"\"\n",
    "    aggr_lu = {\n",
    "        \"hd\": hd,\n",
    "        \"cd\": cd,\n",
    "        \"rx1say\": rx1say,\n",
    "        \"hsd\": hsd,\n",
    "    }\n",
    "    new_da = (\n",
    "        da.transpose(\"time\", \"lat\", \"lon\")\n",
    "        .groupby(\"time.year\")\n",
    "        .reduce(aggr_lu[aggr])\n",
    "        .reset_coords([\"longitude\", \"latitude\", \"height\"], drop=True)\n",
    "    )\n",
    "    new_da.name = aggr\n",
    "    \n",
    "    # add model and scenario coordinate dimensions to the data array\n",
    "    coords_di = {\n",
    "        \"model\": model,\n",
    "        \"scenario\": scenario,\n",
    "    }\n",
    "\n",
    "    new_dims = list(coords_di.keys())\n",
    "    new_da = new_da.assign_coords(coords_di).expand_dims(new_dims)\n",
    "    \n",
    "    return new_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7566a4f-964f-4e24-9f45-240e0debc269",
   "metadata": {},
   "source": [
    "And a wrapper function that will read in the data and run as many extreme variable summaries as requested for that particular model variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68536fb0-4ec4-43c0-84e9-12bee6351daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "def run_summary(fp, aggr_varnames, varname, scenario, model):\n",
    "    \"\"\"Read in data and run all requeste aggregations for\n",
    "    a particular model variable, scenario, and model.\n",
    "    \n",
    "    Args:\n",
    "        fp (path-like): path to the file for the variable\n",
    "            required for creating the aggregate variables\n",
    "            in aggr_varnames\n",
    "        aggr_varnames (list): aggregate variables to derive\n",
    "            using data in provided filepath\n",
    "        varname (str): model variable being used for aggr_varnames\n",
    "        scenario (str): scenario being run\n",
    "        model (str): model being run\n",
    "        \n",
    "    Returns:\n",
    "        summary_das (tuple): tuple of the form (da, aggr_varname, scenario, model),\n",
    "            where da is a DataArray with dimensions of year\n",
    "            (summary year), latitude (lat) and longitude (lon)\n",
    "    \"\"\"\n",
    "    # passing in model, scenario, agregate variable name\n",
    "    #  so this information can be handed back after\n",
    "    #  pool-ing to then construct new DataSet\n",
    "    \n",
    "    ds = xr.load_dataset(fp)\n",
    "    da = ds[varname]\n",
    "        \n",
    "    # convert snow and precip to cm\n",
    "    if varname in [\"pr\", \"prsn\"]:\n",
    "        da = da * 8640\n",
    "        # convert precip to mm\n",
    "        if varname == \"pr\":\n",
    "            da = da * 10\n",
    "        \n",
    "    if varname in [\"tasmax\", \"tasmin\"]:\n",
    "        da = da - 273.15\n",
    "\n",
    "    out = [summarize(da, aggr, model, scenario) for aggr in aggr_varnames]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125d065-6e07-4ab7-ae83-c2911f5fd184",
   "metadata": {},
   "source": [
    "Iterate over the arguments created for each summary and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a71d630-adbd-4f61-ae97-324c86231358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c45dad8a76c4b2ea0072ffa5711723f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "results = []\n",
    "for arg in tqdm(args):\n",
    "    results.extend(run_summary(*arg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ccbe6-df9c-408d-82eb-4ba68a488c5e",
   "metadata": {},
   "source": [
    "Merge the `DataArray`s into one `DataSet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d77ab45f-d397-4db8-902c-2bb8a6bfc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge([da for da in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0803a7-b51d-4906-adfa-3e9ace2fc67d",
   "metadata": {},
   "source": [
    "Set reasonable data types, converting `nan`s to -9999 as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a54bb8c3-2ec5-4abb-9da1-4e753a1892fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan(da):\n",
    "    da.values[np.isnan(da.values)] = -9999\n",
    "    return da\n",
    "\n",
    "\n",
    "ds[\"hsd\"] = replace_nan(ds[\"hsd\"]).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd89042-a0ac-4dc6-8e04-95d838a8832b",
   "metadata": {},
   "source": [
    "Round to reasonable precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5acc0481-d9ad-4f5c-ac03-e99bde0450e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"hd\"] = np.round(ds[\"hd\"], 1)\n",
    "ds[\"cd\"] = np.round(ds[\"cd\"], 1)\n",
    "ds[\"rx1say\"] = np.round(ds[\"rx1say\"], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812cfa2-41fa-40b7-b4e7-910938b10045",
   "metadata": {},
   "source": [
    "Add metadata as attributes of the `DataArray`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff4b3aa0-df2d-40f1-a097-bb1f1720e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"hd\"].attrs = {\n",
    "    \"units\": \"C\",\n",
    "    \"comment\": \"'hot day': 6th hottest day of the year\",\n",
    "}\n",
    "ds[\"cd\"].attrs = {\n",
    "    \"units\": \"C\",\n",
    "    \"comment\": \"'cold day': 6th coldest day of the year\",\n",
    "}\n",
    "ds[\"rx1say\"].attrs = {\n",
    "    \"units\": \"mm\",\n",
    "    \"comment\": \"maximum precipitation for the year\",\n",
    "}\n",
    "ds[\"hsd\"].attrs = {\n",
    "    \"units\": \"day\",\n",
    "    \"comment\": \"number of days exceeding 10cm snowfall\",\n",
    "    \"_FillValue\": -9999,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b22f09-a2f6-46b1-9d7a-d015eda33705",
   "metadata": {},
   "source": [
    "Global metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "35b4e673-33c6-450e-a61c-92d28b6905b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "ds.attrs = {\n",
    "    \"creation_date\": datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15973a9f-a908-49f7-b6cf-339daf903796",
   "metadata": {},
   "source": [
    "Write to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "63202bbc-1bba-44fb-ad85-811ba01b3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(out_ds_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a0fda-afd2-40d8-8188-43c6cbc096a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
