{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988f67bc-1803-46f1-8c97-eb51c850a0c7",
   "metadata": {},
   "source": [
    "# Process extreme weather dataset\n",
    "\n",
    "This notebook is used to derive a dataset of extreme events using the bias-corrected CORDEX data provided to Two Bears Environmental Consulting (via KR at SNAP) by Stantec, Inc.\n",
    "\n",
    "This dataset will consist of four \"extreme\" variables (to start), derived at the annual scale for all available models and scenarios on the same grid as the original bias-corrected data.\n",
    "\n",
    "The dataset will be a datacube with the following dimensions:\n",
    "\n",
    "* variable\n",
    "* model\n",
    "* scenario\n",
    "* year\n",
    "* Y\n",
    "* X\n",
    "\n",
    "## Variables\n",
    "\n",
    "The following variables are to be derived, at the annual scale:\n",
    "\n",
    "* `hd`:  “Hot day” threshold -- the highest observed daily $T_{max}$ such that there are 5 other observations equal to or greater than this value.\n",
    "* `cd`: “Cold day” threshold -- the lowest observed daily $T_{max}$ such that there are 5 other observations equal to or less than this value.\n",
    "* `rx1day`: Max 1-day precipitation –- maximum 1-day precipitation\n",
    "* `hsd`: Heavy Snow Days –- number of days with snowfall > 10 cm\n",
    "\n",
    "## Models\n",
    "\n",
    "The CORDEX data are created by combining a regional climate model with a global circulation model, and there are a couple different types of each represented in this dataset. The combinations are nowhere near exhaustive though, so for our purposes, it should be sufficient to just treat each unique combination as its own \"model\", of which there are 11:\n",
    "\n",
    "* CCCma-CanESM2 x CCCma-CanRCM4\n",
    "* CCCma-CanESM2 x SMHI-RCA4\n",
    "* CCCma-CanESM2 x UQAM-CRCM5\n",
    "* ICHEC-EC-EARTH x DMI-HIRHAM5\n",
    "* ICHEC-EC-EARTH x SMHI-RCA4\n",
    "* ICHEC-EC-EARTH x SMHI-RCA4-SN\n",
    "* MPI-M-MPI-ESM-LR x MGO-RRCM\n",
    "* MPI-M-MPI-ESM-LR x SMHI-RCA4\n",
    "* MPI-M-MPI-ESM-LR x SMHI-RCA4-SN\n",
    "* MPI-M-MPI-ESM-MR x UQAM-CRCM5\n",
    "* NCC-NorESM1-M x SMHI-RCA4\n",
    "\n",
    "## Processing\n",
    "\n",
    "Here we now derive this dataset. The strategy will be to iterate over the datasets and read / summarize into summary `xarray.DataArray` objects with matching dimensions, and then combined into one `xarray.DataSet` to then be saved as a netCDF. \n",
    "\n",
    "Note - optimizations were attempted with `multiprocessing.Pool` but things seemed to be locking up.\n",
    "\n",
    "Run the cell below to import the config file which sets paths to directories, makes common imports, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c690b43d-fd4a-41b4-85e3-9a19aabe292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e0457-0db7-44cd-90b3-9af1d905ec46",
   "metadata": {},
   "source": [
    "Create a list of arguments for filenames and requested summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7f792c-c633-400f-86c2-1eb99a25af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chunk is an artifact of multiprocessing-based optimization attempts\n",
    "#  but it still serves nicely for utilizing a tqdm progress bar in serial processing\n",
    "args = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for varname in varnames:\n",
    "        for model in models:\n",
    "            fp = cordex_dir.joinpath(scenario, varname, temp_fn.format(scenario, varname, model))\n",
    "            \n",
    "            # aggregate variable names for this particular file\n",
    "            aggr_varnames = aggr_varname_lu[varname]\n",
    "            \n",
    "            if fp.exists():\n",
    "                args.append((fp, aggr_varnames, varname, scenario, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fb579-840b-4aad-860b-d79c6af41020",
   "metadata": {},
   "source": [
    "a function for summarizing a `DataArray` of model output for a particular variable to create a new data array of an aggregate variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bea5adf-5001-43e3-a69e-fdaae9dabf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(da, aggr, model, scenario):\n",
    "    \"\"\"Summarize a DataArray according to a specified\n",
    "    aggregation function\n",
    "    \n",
    "    Args:\n",
    "        da (xarray.DataArray): the DataArray object containing the base variable data to b summarized according to aggr\n",
    "        aggr (str): String corresponding to the type of aggregation to do, based on the new aggregation functions\n",
    "        scenario (str): scenario being run (for new coordinate dimension)\n",
    "        model (str): model being run (for new coordinate dimension)\n",
    "            \n",
    "    Returns:\n",
    "        A new data array with dimensions year, latitude, longitude, in that order containing the summarized information\n",
    "    \"\"\"\n",
    "    new_da = (\n",
    "        indices.aggr_func_lu[aggr](da)\n",
    "        .transpose(\"time\", \"lat\", \"lon\")\n",
    "        .reset_coords([\"longitude\", \"latitude\", \"height\"], drop=True)\n",
    "    )\n",
    "    new_da.name = aggr\n",
    "    \n",
    "    # add model and scenario coordinate dimensions to the data array\n",
    "    coords_di = {\n",
    "        \"model\": model,\n",
    "        \"scenario\": scenario,\n",
    "    }\n",
    "\n",
    "    new_dims = list(coords_di.keys())\n",
    "    new_da = new_da.assign_coords(coords_di).expand_dims(new_dims)\n",
    "    \n",
    "    return new_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7566a4f-964f-4e24-9f45-240e0debc269",
   "metadata": {},
   "source": [
    "And a wrapper function that will read in the data and run as many extreme variable summaries as requested for that particular model variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68536fb0-4ec4-43c0-84e9-12bee6351daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_summary(args):\n",
    "    \"\"\"Read in data and run all requested aggregations for a particular model variable, scenario, and model.\n",
    "    \n",
    "    Args:\n",
    "        fp (path-like): path to the file for the variable required for creating the aggregate variables in aggr_varnames\n",
    "        aggr_varnames (list): aggregate variables to derive using data in provided filepath\n",
    "        varname (str): model variable being used for aggr_varnames\n",
    "        scenario (str): scenario being run\n",
    "        model (str): model being run\n",
    "        \n",
    "    Returns:\n",
    "        summary_das (tuple): tuple of the form (da, aggr_varname, scenario, model), where da is a DataArray with dimensions of year (summary year), latitude (lat) and longitude (lon)\n",
    "    \"\"\"\n",
    "    fp, aggr_varnames, varname, scenario, model = args\n",
    "    # passing in model, scenario, agregate variable name\n",
    "    #  so this information can be handed back after\n",
    "    #  pool-ing to then construct new Dataset\n",
    "    \n",
    "    with xr.open_dataset(fp) as ds:\n",
    "        out = [summarize(ds[varname], aggr, model, scenario) for aggr in aggr_varnames]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125d065-6e07-4ab7-ae83-c2911f5fd184",
   "metadata": {},
   "source": [
    "Iterate over the arguments created for each summary and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71d630-adbd-4f61-ae97-324c86231358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████████████████▏                                      | 65/105 [1:16:43<57:44, 86.62s/it]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "results = []\n",
    "# for arg in tqdm.tqdm(args):\n",
    "#     results.extend(run_summary(*arg))\n",
    "    \n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool(8) as pool:\n",
    "    for summary_da in tqdm.tqdm(\n",
    "        pool.imap_unordered(run_summary, args), total=len(args)\n",
    "    ):\n",
    "        results.append(summary_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ccbe6-df9c-408d-82eb-4ba68a488c5e",
   "metadata": {},
   "source": [
    "Merge the `DataArray`s into one `DataSet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d77ab45f-d397-4db8-902c-2bb8a6bfc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge([da for da in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0803a7-b51d-4906-adfa-3e9ace2fc67d",
   "metadata": {},
   "source": [
    "Set reasonable data types, converting `nan`s to -9999 as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a54bb8c3-2ec5-4abb-9da1-4e753a1892fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan(da):\n",
    "    da.values[np.isnan(da.values)] = -9999\n",
    "    return da\n",
    "\n",
    "\n",
    "ds[\"hsd\"] = replace_nan(ds[\"hsd\"]).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd89042-a0ac-4dc6-8e04-95d838a8832b",
   "metadata": {},
   "source": [
    "Round to reasonable precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5acc0481-d9ad-4f5c-ac03-e99bde0450e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"hd\"] = np.round(ds[\"hd\"], 1)\n",
    "ds[\"cd\"] = np.round(ds[\"cd\"], 1)\n",
    "ds[\"rx1day\"] = np.round(ds[\"rx1say\"], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812cfa2-41fa-40b7-b4e7-910938b10045",
   "metadata": {},
   "source": [
    "Add metadata as attributes of the `DataArray`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff4b3aa0-df2d-40f1-a097-bb1f1720e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"hd\"].attrs = {\n",
    "    \"units\": \"C\",\n",
    "    \"comment\": \"'hot day': 6th hottest day of the year\",\n",
    "}\n",
    "ds[\"cd\"].attrs = {\n",
    "    \"units\": \"C\",\n",
    "    \"comment\": \"'cold day': 6th coldest day of the year\",\n",
    "}\n",
    "ds[\"rx1day\"].attrs = {\n",
    "    \"units\": \"mm\",\n",
    "    \"comment\": \"maximum precipitation for the year\",\n",
    "}\n",
    "ds[\"hsd\"].attrs = {\n",
    "    \"units\": \"day\",\n",
    "    \"comment\": \"number of days exceeding 10cm snowfall\",\n",
    "    \"_FillValue\": -9999,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b22f09-a2f6-46b1-9d7a-d015eda33705",
   "metadata": {},
   "source": [
    "Global metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "35b4e673-33c6-450e-a61c-92d28b6905b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "ds.attrs = {\n",
    "    \"creation_date\": datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15973a9f-a908-49f7-b6cf-339daf903796",
   "metadata": {},
   "source": [
    "Write to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "63202bbc-1bba-44fb-ad85-811ba01b3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(out_ds_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a0fda-afd2-40d8-8188-43c6cbc096a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
