{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988f67bc-1803-46f1-8c97-eb51c850a0c7",
   "metadata": {},
   "source": [
    "# Process weather indices dataset\n",
    "\n",
    "This notebook is used to derive a dataset of different indices / extreme events using the bias-corrected CORDEX data provided to Two Bears Environmental Consulting (via KR at SNAP) by Stantec, Inc.\n",
    "\n",
    "This dataset will consist of four indices (to start), derived at the annual scale for all available models and scenarios on the same grid as the original bias-corrected CORDEX data.\n",
    "\n",
    "This dataset will be stored in a netCDF file consisting of a dataset for each variable with the following dimensions:\n",
    "\n",
    "* model\n",
    "* scenario\n",
    "* year\n",
    "* Y\n",
    "* X\n",
    "\n",
    "## Indices\n",
    "\n",
    "The following indices are to be derived, at the annual scale:\n",
    "\n",
    "* `hd`:  “Hot day” threshold -- the highest observed daily $T_{max}$ such that there are 5 other observations equal to or greater than this value.\n",
    "* `cd`: “Cold day” threshold -- the lowest observed daily $T_{min}$ such that there are 5 other observations equal to or less than this value.\n",
    "* `rx1day`: Maximum 1-day precipitation\n",
    "* `hsd`: Heavy Snow Days –- the mean of the snow totals for the 5 snowiest days\n",
    "* `su`: Summer Days –- Annual number of days with Tmax above 25 C\n",
    "* `dw`: Deep Winter days –- Annual number of days with Tmin below -30 C\n",
    "* `wsdi`: Warm Spell Duration Index -- Annual count of occurrences of at least 5 consecutive days with daily mean T above 90 th percentile of historical values for the date\n",
    "* `cdsi`: Cold Spell Duration Index -- Same as WDSI, but for daily mean T below 10 th percentile\n",
    "* `rx5day`: Maximum 5-day precipitation\n",
    "* `r10mm`: Number of heavy precip days –- Annual count of days with precip > 10 mm\n",
    "* `cwd`: Consecutive wet days –- Yearly number of the most consecutive days with precip > 1 mm\n",
    "* `cdd`: Consecutive dry days –- Same as CED, but for days with precip < 1 mm\n",
    "* `wndd`: Windy Days – Yearly number of days with mean wind speed > 10 m/sec\n",
    "\n",
    "## Models\n",
    "\n",
    "The CORDEX data are created by combining a regional climate model with a global circulation model, and there are a couple different types of each represented in this dataset. The combinations are nowhere near exhaustive though, so for our purposes, it should be sufficient to just treat each unique combination as its own \"model\", of which there are 11:\n",
    "\n",
    "* CCCma-CanESM2 x CCCma-CanRCM4\n",
    "* CCCma-CanESM2 x SMHI-RCA4\n",
    "* CCCma-CanESM2 x UQAM-CRCM5\n",
    "* ICHEC-EC-EARTH x DMI-HIRHAM5\n",
    "* ICHEC-EC-EARTH x SMHI-RCA4\n",
    "* ICHEC-EC-EARTH x SMHI-RCA4-SN\n",
    "* MPI-M-MPI-ESM-LR x MGO-RRCM\n",
    "* MPI-M-MPI-ESM-LR x SMHI-RCA4\n",
    "* MPI-M-MPI-ESM-LR x SMHI-RCA4-SN\n",
    "* MPI-M-MPI-ESM-MR x UQAM-CRCM5\n",
    "* NCC-NorESM1-M x SMHI-RCA4\n",
    "\n",
    "## Processing\n",
    "\n",
    "Here we now derive this dataset. The strategy will be to iterate over the datasets and read / summarize into summary `xarray.DataArray` objects with matching dimensions, and then combined into one `xarray.Dataset` to then be saved as a netCDF. \n",
    "\n",
    "Run the cell below to import the config file which sets paths to directories, makes common imports, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c690b43d-fd4a-41b4-85e3-9a19aabe292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "# project\n",
    "from config import *\n",
    "import indices\n",
    "# ignore all-nan slice warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "import time\n",
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e0457-0db7-44cd-90b3-9af1d905ec46",
   "metadata": {},
   "source": [
    "Create a list of arguments for filenames and requested summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7f792c-c633-400f-86c2-1eb99a25af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chunk is an artifact of multiprocessing-based optimization attempts\n",
    "#  but it still serves nicely for utilizing a tqdm progress bar in serial processing\n",
    "args = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for varname in varnames:\n",
    "        for model in models:\n",
    "            fp = cordex_dir.joinpath(scenario, varname, temp_fn.format(scenario, varname, model))\n",
    "            \n",
    "            # aggregate variable names for this particular file\n",
    "            idx_varnames = idx_varname_lu[varname]\n",
    "            \n",
    "            # not all combinations of model, scenario, and model variable actually exist\n",
    "            if fp.exists():\n",
    "                args.append((fp, idx_varnames, varname, scenario, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7566a4f-964f-4e24-9f45-240e0debc269",
   "metadata": {},
   "source": [
    "We will use functions from the `indices.py` script to derive the indices. Define a wrapper function for the `compute_index` function that will open the connection to a dataset (modeled climate variable) and compute all requested indices for that particular file.\n",
    "\n",
    "Note - currently, there is only one index computed per variable, but this approach was chosen to facilitate addition of multiple indices per variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68536fb0-4ec4-43c0-84e9-12bee6351daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_compute_index(args):\n",
    "    \"\"\"Read in data and compute all requested indices for a particular model variable, scenario, and model.\n",
    "    \n",
    "    Args:\n",
    "        fp (path-like): path to the file for the variable required for creating the index variables in indices\n",
    "        index_list (list): indices to derive using data in provided filepath\n",
    "        varname (str): model variable being used for indices\n",
    "        scenario (str): scenario being run\n",
    "        model (str): model being run\n",
    "        \n",
    "    Returns:\n",
    "        summary_das (tuple): tuple of the form (da, index, scenario, model), where da is a DataArray with dimensions of year (summary year), latitude (lat) and longitude (lon)\n",
    "    \"\"\"\n",
    "    fp, index_list, varname, scenario, model = args\n",
    "    # passing in model, scenario, agregate variable name\n",
    "    #  so this information can be handed back after\n",
    "    #  pool-ing to then construct new Dataset\n",
    "    \n",
    "    with xr.open_dataset(fp) as ds:\n",
    "        out = []\n",
    "        for index in index_list:\n",
    "            if index in [\"wsdi\", \"csdi\"]:\n",
    "                # for these special indices we need to derive percentiles\n",
    "                #  from the historical data\n",
    "                hist_fp = str(fp).replace(scenario, \"hist\")\n",
    "                with xr.open_dataset(hist_fp) as hist_ds:\n",
    "                    kwargs = {\"hist_da\": hist_ds[varname]}\n",
    "                    out.append(indices.compute_index(ds[varname], index, model, scenario, kwargs))\n",
    "            else:\n",
    "                out.append(indices.compute_index(ds[varname], index, model, scenario))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125d065-6e07-4ab7-ae83-c2911f5fd184",
   "metadata": {},
   "source": [
    "Iterate over the arguments created for each index and run. Looks like this seems to be taking ~6 minutes on Atlas using 32 cores if the CORDEX data is available on scratch space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a71d630-adbd-4f61-ae97-324c86231358",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 133/133 [07:48<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "    \n",
    "# using less than max number of cpus on Atlas nodes helps with memory allocation errors\n",
    "with Pool(15) as pool:\n",
    "    for summary_da in tqdm.tqdm(\n",
    "        pool.imap_unordered(run_compute_index, args), total=len(args)\n",
    "    ):\n",
    "        results.append(summary_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ccbe6-df9c-408d-82eb-4ba68a488c5e",
   "metadata": {},
   "source": [
    "Merge the `DataArray`s into one `DataSet` (this might take a couple minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77ab45f-d397-4db8-902c-2bb8a6bfc95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 43s, sys: 4min 5s, total: 9min 48s\n",
      "Wall time: 9min 48s\n"
     ]
    }
   ],
   "source": [
    "%time ds = xr.merge([da for da_list in results for da in da_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0803a7-b51d-4906-adfa-3e9ace2fc67d",
   "metadata": {},
   "source": [
    "Set reasonable data types, converting `nan`s to -9999 for the integer types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54bb8c3-2ec5-4abb-9da1-4e753a1892fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan(da):\n",
    "    da.values[np.isnan(da.values)] = -9999\n",
    "    da.attrs[\"_FillValue\"] = -9999\n",
    "    return da\n",
    "\n",
    "\n",
    "for varname in [\"r10mm\", \"wsdi\", \"csdi\", \"cwd\", \"cdd\", \"su\", \"dw\", \"wndd\"]:\n",
    "    ds[varname] = replace_nan(ds[varname]).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd89042-a0ac-4dc6-8e04-95d838a8832b",
   "metadata": {},
   "source": [
    "Round to reasonable precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5acc0481-d9ad-4f5c-ac03-e99bde0450e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for varname in [\"hsd\", \"hd\", \"cd\", \"rx1day\", \"rx5day\"]:\n",
    "    ds[varname] = np.round(ds[varname], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b22f09-a2f6-46b1-9d7a-d015eda33705",
   "metadata": {},
   "source": [
    "Global metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b4e673-33c6-450e-a61c-92d28b6905b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "ds.attrs = {\n",
    "    \"creation_date\": datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15973a9f-a908-49f7-b6cf-339daf903796",
   "metadata": {},
   "source": [
    "Write to disk (might take a couple of minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63202bbc-1bba-44fb-ad85-811ba01b3b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 3.09 s, total: 3.25 s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%time ds.to_netcdf(indices_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f14cdd88-690a-4661-9b1e-1e7e569127d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook completed in 21.5m\n"
     ]
    }
   ],
   "source": [
    "# if running as script\n",
    "print(f\"This notebook completed in {round((time.perf_counter() - tic) / 60, 1)}m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
